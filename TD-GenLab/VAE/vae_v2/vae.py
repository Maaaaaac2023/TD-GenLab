import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, TensorDataset
import os
import time

# é…ç½®å¸¸é‡
BASE_DIR = 'vae'
MODEL_PATH = os.path.join(BASE_DIR, 'cvae_model.pth')
TRAINING_LOSS_PATH = os.path.join(BASE_DIR, 'training_loss.png')

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(BASE_DIR, exist_ok=True)

# 2. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
def load_and_preprocess_data(difficulty_matrix):
    """
    åŠ è½½å’Œé¢„å¤„ç†å…³å¡æ•°æ®ï¼Œä½¿ç”¨æ ‡å‡†åŒ–æ³¢æ¬¡è€Œéæ³¢æ¬¡å€’æ•°
    """
    # åŠ è½½å…³å¡æ•°æ®
    level_files = [
        os.path.join(BASE_DIR, 'Level_1_Summary.csv'),
        os.path.join(BASE_DIR, 'Level_2_Summary.csv'), 
        os.path.join(BASE_DIR, 'Level_3_Summary.csv')
    ]
    
    # æå–ç‰¹å¾ (æ’é™¤Wave Numberå’ŒWave Intervalåˆ—)
    features = ['goblin', 'goblin_priest', 'skeleton', 'slime', 'slime_king', 'Coin Reward']
    
    # æ„å»ºæ•°æ®é›†
    all_data = []
    all_difficulties = []
    all_wave_numbers = []
    
    for i, level_file in enumerate(level_files):
        if not os.path.exists(level_file):
            print(f"âš ï¸  è­¦å‘Š: æ–‡ä»¶ä¸å­˜åœ¨ {level_file}")
            continue
            
        level = pd.read_csv(level_file)
        
        # æ¯ä¸ªå…³å¡å–8æ³¢æ•°æ®
        wave_data = level[features].values[:8]
        all_data.append(wave_data)
        
        # ä½¿ç”¨æ–°çš„éš¾åº¦çŸ©é˜µ
        difficulties = difficulty_matrix[i].reshape(-1, 1)
        all_difficulties.append(difficulties)
        
        # ä½¿ç”¨æ ‡å‡†åŒ–æ³¢æ¬¡ (0-1)
        wave_numbers = np.arange(1, 9).reshape(-1, 1)
        normalized_wave_numbers = (wave_numbers - 1) / 7  # å½’ä¸€åŒ–åˆ°[0,1]
        all_wave_numbers.append(normalized_wave_numbers)
    
    if not all_data:
        raise FileNotFoundError("æœªæ‰¾åˆ°ä»»ä½•å…³å¡æ•°æ®æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥vaeæ–‡ä»¶å¤¹ä¸­çš„CSVæ–‡ä»¶")
    
    # åˆå¹¶æ•°æ®
    X = np.vstack(all_data)  # å½¢çŠ¶: (24, 6)
    y_difficulty = np.vstack(all_difficulties)  # å½¢çŠ¶: (24, 1)
    y_wave_number = np.vstack(all_wave_numbers)  # å½¢çŠ¶: (24, 1)
    
    # æ ‡å‡†åŒ–æ•°æ®
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)
    
    return X_scaled, y_difficulty, y_wave_number, scaler, features

# 3. CVAE æ¨¡å‹å®šä¹‰
class CVAE(nn.Module):
    def __init__(self, input_dim, condition_dim, latent_dim=12):
        super(CVAE, self).__init__()
        
        # ç¼–ç å™¨
        self.encoder = nn.Sequential(
            nn.Linear(input_dim + condition_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU()
        )
        
        # æ½œåœ¨ç©ºé—´å‚æ•°
        self.fc_mu = nn.Linear(64, latent_dim)
        self.fc_logvar = nn.Linear(64, latent_dim)
        
        # è§£ç å™¨
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim + condition_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, input_dim),
            nn.Sigmoid()
        )
    
    def encode(self, x, c):
        inputs = torch.cat([x, c], dim=1)
        h = self.encoder(inputs)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z, c):
        inputs = torch.cat([z, c], dim=1)
        return self.decoder(inputs)
    
    def forward(self, x, c):
        mu, logvar = self.encode(x, c)
        z = self.reparameterize(mu, logvar)
        return self.decode(z, c), mu, logvar

# 4. éš¾åº¦æ›²çº¿æ§åˆ¶å™¨
def apply_difficulty_curve(generated_data, base_difficulty, wave_numbers):
    """
    åº”ç”¨æ˜¾å¼çš„éš¾åº¦æ›²çº¿æ§åˆ¶ï¼Œç¡®ä¿ç”Ÿæˆçš„å…³å¡ç¬¦åˆæ¸¸æˆè®¾è®¡åŸåˆ™
    """
    num_waves = generated_data.shape[0]
    
    # é€‰æ‹©éš¾åº¦æ›²çº¿ç±»å‹ï¼ˆæ ¹æ®åŸºç¡€éš¾åº¦ï¼‰
    if base_difficulty < 0.3:
        curve_type = "linear_gentle"   # ç®€å•å…³å¡ï¼šå¹³ç¼“å¢é•¿
    elif base_difficulty < 0.6:
        curve_type = "linear_medium"   # ä¸­ç­‰å…³å¡ï¼šä¸­ç­‰å¢é•¿
    else:
        curve_type = "exponential"     # å›°éš¾å…³å¡ï¼šæŒ‡æ•°å¢é•¿
    
    # è®¡ç®—éš¾åº¦ç³»æ•°
    difficulty_factors = np.zeros(num_waves)
    
    for i, wave_num in enumerate(range(1, num_waves + 1)):
        # æ ¹æ®æ›²çº¿ç±»å‹è®¡ç®—éš¾åº¦ç³»æ•°
        normalized_wave = (wave_num - 1) / (num_waves - 1)  # 0-1èŒƒå›´
        
        if curve_type == "linear_gentle":
            # ç®€å•å…³å¡ï¼šå¹³ç¼“å¢é•¿ (0.6 â†’ 1.0)
            factor = 0.6 + 0.4 * normalized_wave
        elif curve_type == "linear_medium":
            # ä¸­ç­‰å…³å¡ï¼šä¸­ç­‰å¢é•¿ (0.5 â†’ 1.2)
            factor = 0.5 + 0.7 * normalized_wave
        else:  # "exponential"
            # å›°éš¾å…³å¡ï¼šæŒ‡æ•°å¢é•¿ï¼ŒåæœŸæ€¥å‰§ä¸Šå‡
            factor = 0.4 + 1.0 * (normalized_wave ** 1.5)
        
        # æ·»åŠ å°çš„éšæœºæ³¢åŠ¨ï¼Œä½¿æ›²çº¿æ›´è‡ªç„¶
        noise = np.random.uniform(0.95, 1.05)
        difficulty_factors[i] = factor * noise
    
    # åº”ç”¨éš¾åº¦ç³»æ•°åˆ°æ€ªç‰©æ•°é‡
    for wave_idx in range(num_waves):
        factor = difficulty_factors[wave_idx]
        
        # å“¥å¸ƒæ—ã€å“¥å¸ƒæ—ç¥­å¸ã€éª·é«…ã€å²è±å§†ã€å²è±å§†ç‹
        for monster_idx in range(5):
            # å¢åŠ æ€ªç‰©æ•°é‡ï¼Œä½†ä¿ç•™ä¸€äº›å˜åŒ–
            generated_data[wave_idx, monster_idx] *= factor
            
            # ç¡®ä¿å²è±å§†ç‹åªåœ¨åæœŸå‡ºç°ï¼ˆæ¸¸æˆè®¾è®¡åŸåˆ™ï¼‰
            if monster_idx == 4 and wave_idx < 3:  # å‰3æ³¢
                generated_data[wave_idx, monster_idx] *= 0.3  # å‡å°‘å²è±å§†ç‹æ•°é‡
        
        # é‡‘å¸å¥–åŠ±åº”éšéš¾åº¦å¢åŠ 
        generated_data[wave_idx, 5] = max(50, generated_data[wave_idx, 5] * (0.7 + 0.5 * factor))
    
    return generated_data

# 5. è®­ç»ƒå’Œä¿å­˜æ¨¡å‹å‡½æ•°
def train_and_save_cvae(model, dataloader, optimizer, epochs=300):
    model.train()
    losses = []
    
    print("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒCVAEæ¨¡å‹...")
    start_time = time.time()
    
    for epoch in range(epochs):
        total_loss = 0
        for x_batch, c_batch in dataloader:
            recon_x, mu, logvar = model(x_batch, c_batch)
            
            # è®¡ç®—æŸå¤±
            recon_loss = nn.MSELoss()(recon_x, x_batch)
            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
            
            loss = recon_loss + 0.005 * kl_loss
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        avg_loss = total_loss / len(dataloader)
        losses.append(avg_loss)
        
        if (epoch + 1) % 50 == 0:
            elapsed = time.time() - start_time
            print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}, è€—æ—¶: {elapsed:.1f}s')
    
    # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
    torch.save(model.state_dict(), MODEL_PATH)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³ {MODEL_PATH}")
    
    return losses

# 6. ç”Ÿæˆæ–°å…³å¡
def generate_level(model, scaler, difficulty, num_waves=8):
    """
    ç”Ÿæˆç¬¦åˆéš¾åº¦é€’å¢åŸåˆ™çš„æ–°å…³å¡
    """
    model.eval()
    
    # 1. åˆ›å»ºæ¡ä»¶å˜é‡
    difficulties = torch.full((num_waves, 1), difficulty, dtype=torch.float32)
    wave_numbers = torch.linspace(0, 1, num_waves, dtype=torch.float32).reshape(-1, 1)
    
    conditions = torch.cat([difficulties, wave_numbers], dim=1)
    
    # 2. ç”ŸæˆåŸºç¡€æ•°æ®
    with torch.no_grad():
        z = torch.randn(num_waves, model.fc_mu.out_features)
        generated = model.decode(z, conditions).numpy()
    
    # 3. åæ ‡å‡†åŒ–
    generated_original = scaler.inverse_transform(generated)
    
    # 4. åº”ç”¨æ˜¾å¼éš¾åº¦æ›²çº¿
    generated_original = apply_difficulty_curve(generated_original, difficulty, wave_numbers)
    
    # 5. åå¤„ç†
    for i in range(5):  # å‰5åˆ—æ˜¯æ€ªç‰©æ•°é‡
        generated_original[:, i] = np.round(np.maximum(generated_original[:, i], 0))
        
        # ç‰¹æ®Šè§„åˆ™ï¼šç¬¬1æ³¢ä¸èƒ½æœ‰å²è±å§†ç‹æˆ–å“¥å¸ƒæ—ç¥­å¸
        if i in [1, 4]:  # å“¥å¸ƒæ—ç¥­å¸(1)å’Œå²è±å§†ç‹(4)
            generated_original[0, i] = 0
    
    # ç¡®ä¿Coin Rewardä¸º10çš„å€æ•°
    generated_original[:, 5] = np.round(generated_original[:, 5] / 10) * 10
    generated_original[:, 5] = np.maximum(generated_original[:, 5], 50)  # æœ€ä½50é‡‘å¸
    
    # æ·»åŠ Wave Intervalåˆ—
    if difficulty < 0.3:
        base_interval = 5.0  # ç®€å•å…³å¡
    elif difficulty < 0.6:
        base_interval = 4.0  # ä¸­ç­‰å…³å¡
    else:
        base_interval = 3.0  # å›°éš¾å…³å¡
    
    wave_intervals = np.full((num_waves, 1), base_interval)
    wave_intervals[-1] = 0  # æœ€åä¸€æ³¢é—´éš”ä¸º0
    
    # æ·»åŠ Wave Numberåˆ—
    wave_numbers_np = np.arange(1, num_waves + 1).reshape(-1, 1)
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    final_level_data = np.hstack([wave_numbers_np, generated_original, wave_intervals])
    
    return final_level_data

# 7. ä¿å­˜å’Œå¯è§†åŒ–ç”Ÿæˆçš„å…³å¡
def save_and_visualize_level(level_data, features, difficulty, base_dir=BASE_DIR):
    filename = os.path.join(base_dir, f'Generated_Level_Diff_{difficulty:.2f}.csv')
    
    # åˆ›å»ºDataFrame
    all_columns = ['Wave Number'] + features + ['Wave Interval']
    df = pd.DataFrame(level_data, columns=all_columns)
    
    # ä¿å­˜åˆ°CSV
    df.to_csv(filename, index=False)
    print(f'âœ… ç”Ÿæˆçš„å…³å¡å·²ä¿å­˜åˆ° {filename}')
    
    # å¯è§†åŒ–1: æ€ªç‰©åˆ†å¸ƒ
    monster_plot_path = os.path.join(base_dir, f'level_diff_{difficulty:.2f}_monsters.png')
    plt.figure(figsize=(12, 6))
    monster_names = ['goblin', 'skeleton', 'slime', 'slime_king']
    colors = ['blue', 'green', 'red', 'purple']
    
    for i, monster in enumerate(monster_names):
        if monster in features:
            idx = all_columns.index(monster)
            plt.plot(level_data[:, 0], level_data[:, idx], 'o-', 
                     linewidth=2.5, markersize=8, color=colors[i], label=monster)
    
    plt.title(f'ç”Ÿæˆå…³å¡ (åŸºç¡€éš¾åº¦: {difficulty:.2f}) - æ€ªç‰©åˆ†å¸ƒ', fontsize=14)
    plt.xlabel('æ³¢æ¬¡', fontsize=12)
    plt.ylabel('æ€ªç‰©æ•°é‡', fontsize=12)
    plt.xticks(range(1, 9))
    plt.grid(True, alpha=0.3)
    plt.legend(fontsize=12)
    plt.savefig(monster_plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    # å¯è§†åŒ–2: é‡‘å¸å¥–åŠ±
    coin_plot_path = os.path.join(base_dir, f'level_diff_{difficulty:.2f}_coins.png')
    plt.figure(figsize=(10, 5))
    coin_idx = all_columns.index('Coin Reward')
    plt.bar(level_data[:, 0], level_data[:, coin_idx], color='gold', edgecolor='darkgoldenrod', alpha=0.8)
    plt.title(f'ç”Ÿæˆå…³å¡ (åŸºç¡€éš¾åº¦: {difficulty:.2f}) - é‡‘å¸å¥–åŠ±', fontsize=14)
    plt.xlabel('æ³¢æ¬¡', fontsize=12)
    plt.ylabel('é‡‘å¸æ•°é‡', fontsize=12)
    plt.xticks(range(1, 9))
    plt.grid(axis='y', alpha=0.3)
    plt.savefig(coin_plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š å·²ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨: '{os.path.basename(monster_plot_path)}' å’Œ '{os.path.basename(coin_plot_path)}'")
    return df

# 8. éªŒè¯ç”Ÿæˆçš„å…³å¡æ˜¯å¦ç¬¦åˆéš¾åº¦é€’å¢åŸåˆ™
def validate_difficulty_progression(level_data):
    """
    éªŒè¯ç”Ÿæˆçš„å…³å¡æ˜¯å¦ç¬¦åˆéš¾åº¦é€’å¢åŸåˆ™
    """
    print("\nğŸ” éš¾åº¦é€’å¢éªŒè¯åˆ†æ:")
    print("-" * 60)
    
    # è®¡ç®—æ¯æ³¢çš„"éš¾åº¦åˆ†æ•°"ï¼ˆæ€ªç‰©æ•°é‡åŠ æƒå’Œï¼‰
    weights = [1.0, 1.5, 1.0, 0.8, 2.0]  # ä¸åŒæ€ªç‰©çš„éš¾åº¦æƒé‡
    difficulty_scores = []
    
    for wave_idx in range(8):
        score = 0
        for monster_idx, weight in enumerate(weights):
            count = level_data[wave_idx, monster_idx + 1]  # +1è·³è¿‡Wave Number
            score += count * weight
        difficulty_scores.append(score)
    
    # æ‰“å°åˆ†æ
    print("æ³¢æ¬¡ | éš¾åº¦åˆ†æ•° | ä¸å‰ä¸€æ³¢å˜åŒ–")
    print("-" * 40)
    
    for i, score in enumerate(difficulty_scores):
        if i == 0:
            change = "åŸºå‡†"
        else:
            change_percent = (score - difficulty_scores[i-1]) / difficulty_scores[i-1] * 100
            change = f"{change_percent:+.1f}%"
        
        print(f"{i+1:2d} | {score:8.1f} | {change}")
    
    # éªŒè¯éš¾åº¦é€’å¢
    increasing = all(difficulty_scores[i] >= difficulty_scores[i-1] * 0.9 for i in range(1, 8))
    final_vs_first = difficulty_scores[-1] / difficulty_scores[0]
    
    print("-" * 60)
    print(f"âœ… éš¾åº¦æ€»ä½“é€’å¢: {'æ˜¯' if increasing else 'å¦'}")
    print(f"ğŸ“ˆ æœ€ç»ˆæ³¢ vs ç¬¬ä¸€æ³¢: {final_vs_first:.1f}å€éš¾åº¦")
    
    if increasing and final_vs_first > 1.5:
        print("ğŸ‰ ç”Ÿæˆçš„å…³å¡ç¬¦åˆæ¸¸æˆè®¾è®¡åŸåˆ™ï¼")
    else:
        print("âš ï¸  å»ºè®®é‡æ–°ç”Ÿæˆæˆ–è°ƒæ•´éš¾åº¦å‚æ•°")

# ä¸»ç¨‹åº
def main():
    # 1. åˆ›å»ºç¬¦åˆæ¸¸æˆè®¾è®¡åŸåˆ™çš„éš¾åº¦æ•°æ®
    print("ğŸ¯ åˆ›å»ºç¬¦åˆæ¸¸æˆè®¾è®¡åŸåˆ™çš„éš¾åº¦æ›²çº¿æ•°æ®...")
    difficulty_matrix = np.array([
        [0.10, 0.12, 0.15, 0.18, 0.21, 0.24, 0.27, 0.30],  # Level 1
        [0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65],  # Level 2
        [0.50, 0.58, 0.66, 0.74, 0.82, 0.86, 0.90, 0.95]   # Level 3
    ])
    
    # 2. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    print("\nğŸ“Š åŠ è½½å’Œé¢„å¤„ç†å…³å¡æ•°æ®...")
    try:
        X_scaled, y_difficulty, y_wave_number, scaler, features = load_and_preprocess_data(difficulty_matrix)
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
        return
    
    # 3. å‡†å¤‡PyTorchæ•°æ®
    X_tensor = torch.tensor(X_scaled, dtype=torch.float32)
    conditions = np.hstack([y_difficulty, y_wave_number])
    c_tensor = torch.tensor(conditions, dtype=torch.float32)
    
    dataset = TensorDataset(X_tensor, c_tensor)
    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)
    
    # 4. åˆå§‹åŒ–CVAEæ¨¡å‹
    input_dim = X_scaled.shape[1]  # 6ä¸ªç‰¹å¾
    condition_dim = conditions.shape[1]  # 2ä¸ªæ¡ä»¶å˜é‡
    
    print("\nğŸ§  åˆå§‹åŒ–CVAEæ¨¡å‹...")
    model = CVAE(input_dim, condition_dim)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # 5. æ£€æŸ¥æ˜¯å¦å­˜åœ¨é¢„è®­ç»ƒæ¨¡å‹
    if os.path.exists(MODEL_PATH):
        print(f"ğŸ“¥ æ£€æµ‹åˆ°å·²è®­ç»ƒæ¨¡å‹ï¼ŒåŠ è½½ä¸­: {MODEL_PATH}")
        model.load_state_dict(torch.load(MODEL_PATH))
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼è·³è¿‡è®­ç»ƒé˜¶æ®µã€‚")
    else:
        print("\nğŸ†• æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¼€å§‹è®­ç»ƒæ–°æ¨¡å‹...")
        # è®­ç»ƒæ¨¡å‹
        losses = train_and_save_cvae(model, dataloader, optimizer, epochs=400)
        
        # å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
        plt.figure(figsize=(10, 5))
        plt.plot(losses, linewidth=2, color='blue')
        plt.title('CVAE è®­ç»ƒæŸå¤±æ›²çº¿', fontsize=14)
        plt.xlabel('Epoch', fontsize=12)
        plt.ylabel('Loss', fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.savefig(TRAINING_LOSS_PATH, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š è®­ç»ƒæŸå¤±æ›²çº¿å·²ä¿å­˜ä¸º '{TRAINING_LOSS_PATH}'")
    
    # 6. ç”Ÿæˆä¸åŒéš¾åº¦çš„å…³å¡
    target_difficulties = [0.2, 0.5, 0.8]  # ä½ã€ä¸­ã€é«˜éš¾åº¦
    
    for diff in target_difficulties:
        print(f"\nğŸ² ç”ŸæˆåŸºç¡€éš¾åº¦ä¸º {diff:.2f} çš„æ–°å…³å¡ (ç¬¦åˆéš¾åº¦é€’å¢åŸåˆ™)...")
        generated_level = generate_level(model, scaler, diff)
        
        # ä¿å­˜å’Œå¯è§†åŒ–
        df = save_and_visualize_level(generated_level, features, diff)
        
        # éªŒè¯éš¾åº¦é€’å¢
        validate_difficulty_progression(generated_level)
        
        # æ‰“å°è¯¦ç»†æ•°æ®
        print("\nğŸ“‹ ç”Ÿæˆçš„å…³å¡è¯¦ç»†æ•°æ®:")
        print(df.to_string(index=False))
    
    print("\nğŸ‰ å®Œæˆ! æ‰€æœ‰å…³å¡å·²æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜ã€‚")
    print(f"   ç”Ÿæˆçš„æ–‡ä»¶å‡ä¿å­˜åœ¨ '{BASE_DIR}' æ–‡ä»¶å¤¹ä¸­:")
    print(f"   - æ¨¡å‹æ–‡ä»¶: {os.path.basename(MODEL_PATH)}")
    print(f"   - è®­ç»ƒæŸå¤±å›¾: {os.path.basename(TRAINING_LOSS_PATH)}")
    print("   - ç”Ÿæˆçš„å…³å¡æ–‡ä»¶ (3ä¸ªéš¾åº¦çº§åˆ«)")
    print("   - å¯è§†åŒ–å›¾è¡¨ (æ€ªç‰©åˆ†å¸ƒå’Œé‡‘å¸å¥–åŠ±)")

if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§
    torch.manual_seed(42)
    np.random.seed(42)
    
    # è¿è¡Œä¸»ç¨‹åº
    main()